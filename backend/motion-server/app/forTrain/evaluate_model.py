"""
í•™ìŠµëœ ëª¨ë¸ ì¢…í•© í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì „ì²˜ë¦¬ëœ ë°ì´í„°(pose_sequences)ë¥¼ ì‚¬ìš©í•˜ì—¬:
1. Confusion Matrix
2. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ (Accuracy, Precision, Recall, F1)
3. í™•ë¥  ë¶„í¬ ë¶„ì„
4. ì ìˆ˜ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜
5. STAY vs CLAP êµ¬ë¶„ ëŠ¥ë ¥ ë¶„ì„

ì‚¬ìš©ë²•:
    python evaluate_model.py --model checkpoints/gcn_cnn_best.pt --data_dir ./pose_sequences
"""

from __future__ import annotations

import argparse
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support

# train_gcn_cnn.pyì—ì„œ ì •ì˜ëœ ê²ƒë“¤ import
from train_gcn_cnn import (
    SUPPORTED_ACTIONS,
    USED_LANDMARK_INDICES,
    GCNTemporalModel,
    build_adjacency,
    normalize_landmarks,
)


def load_model(model_path: Path, num_classes: int, device: str = "cuda") -> GCNTemporalModel:
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)

    # ëª¨ë¸ êµ¬ì¡° íŒŒë¼ë¯¸í„°
    gcn_hidden_dims = checkpoint.get("args", {}).get("gcn_hidden_dims", [96, 192])
    temporal_channels = checkpoint.get("args", {}).get("temporal_channels", [192, 384])
    dropout = checkpoint.get("args", {}).get("dropout", 0.4)

    # ëª¨ë¸ ìƒì„±
    input_dim = 2  # x, y ì¢Œí‘œ
    adjacency = build_adjacency(USED_LANDMARK_INDICES)

    model = GCNTemporalModel(
        input_dim=input_dim,
        num_classes=num_classes,
        adjacency=adjacency,
        gcn_hidden_dims=gcn_hidden_dims,
        temporal_channels=temporal_channels,
        dropout=dropout,
    )

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint["model_state_dict"])
    model.to(device)
    model.eval()

    return model


def load_all_samples(data_dir: Path, action_to_label: Dict[str, int], frames_per_sample: int = 8):
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì „ì²´ ë¡œë“œ"""
    samples = []

    for person_dir in sorted(p for p in data_dir.iterdir() if p.is_dir()):
        person_name = person_dir.name.upper()

        for action_dir in sorted(p for p in person_dir.iterdir() if p.is_dir()):
            action_name = action_dir.name.upper()

            if action_name not in action_to_label:
                continue

            for npz_file in sorted(action_dir.glob("*.npz")):
                try:
                    with np.load(npz_file, allow_pickle=True) as data:
                        landmarks = data["landmarks"]

                        if landmarks.shape[0] != frames_per_sample:
                            continue

                        # ì •ê·œí™”
                        landmarks = normalize_landmarks(landmarks)

                        samples.append({
                            "data": landmarks,
                            "label": action_to_label[action_name],
                            "action": action_name,
                            "person": person_name,
                            "file": npz_file,
                        })
                except Exception as e:
                    print(f"âš ï¸  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {npz_file}, ì˜¤ë¥˜: {e}")
                    continue

    return samples


def evaluate_model_comprehensive(
    model: GCNTemporalModel,
    samples: List[Dict],
    label_to_action: Dict[int, str],
    device: str = "cuda"
):
    """ì „ì²´ ìƒ˜í”Œ í‰ê°€ ë° ìƒì„¸ ë¶„ì„"""

    all_labels = []
    all_preds = []
    all_probs = []  # ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ 
    all_max_probs = []  # ìµœê³  í™•ë¥ ê°’
    all_target_probs = []  # ëª©í‘œ í´ë˜ìŠ¤ í™•ë¥ 
    misclassified = []

    # í´ë˜ìŠ¤ë³„ í™•ë¥  ì €ì¥ (STAY vs CLAP ë¶„ì„ìš©)
    class_probs = defaultdict(list)

    print("ğŸ” ëª¨ë¸ í‰ê°€ ì¤‘...")

    with torch.no_grad():
        for sample in samples:
            data = torch.from_numpy(sample["data"]).unsqueeze(0).to(device)  # (1, T, N, C)
            label = sample["label"]
            action = sample["action"]

            # ëª¨ë¸ ì˜ˆì¸¡ (logits)
            logits = model(data)

            # Softmaxë¡œ í™•ë¥  ë³€í™˜
            probs = F.softmax(logits, dim=1).cpu().numpy()[0]  # (num_classes,)

            pred = int(np.argmax(probs))
            max_prob = float(np.max(probs))
            target_prob = float(probs[label])

            all_labels.append(label)
            all_preds.append(pred)
            all_probs.append(probs)
            all_max_probs.append(max_prob)
            all_target_probs.append(target_prob)

            # í´ë˜ìŠ¤ë³„ í™•ë¥  ì €ì¥
            class_probs[action].append(probs)

            # ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ì €ì¥
            if pred != label:
                misclassified.append({
                    "true_label": label,
                    "true_action": action,
                    "pred_label": pred,
                    "pred_action": label_to_action[pred],
                    "true_prob": target_prob,
                    "pred_prob": max_prob,
                    "all_probs": probs,
                    "person": sample["person"],
                    "file": sample["file"],
                })

    return {
        "labels": np.array(all_labels),
        "preds": np.array(all_preds),
        "probs": np.array(all_probs),
        "max_probs": np.array(all_max_probs),
        "target_probs": np.array(all_target_probs),
        "misclassified": misclassified,
        "class_probs": class_probs,
    }


def print_evaluation_results(results: Dict, label_to_action: Dict[int, str]):
    """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""

    labels = results["labels"]
    preds = results["preds"]
    probs = results["probs"]
    max_probs = results["max_probs"]
    target_probs = results["target_probs"]
    misclassified = results["misclassified"]
    class_probs = results["class_probs"]

    num_classes = len(label_to_action)
    class_names = [label_to_action[i] for i in range(num_classes)]

    print("\n" + "="*80)
    print("ëª¨ë¸ í‰ê°€ ê²°ê³¼")
    print("="*80)

    # ì „ì²´ ì •í™•ë„
    accuracy = np.mean(labels == preds) * 100
    print(f"\nâ–¶ ì „ì²´ ì„±ëŠ¥:")
    print(f"  - ì´ ìƒ˜í”Œ: {len(labels)}ê°œ")
    print(f"  - ì •í™•ë„: {accuracy:.2f}% ({int(np.sum(labels == preds))}/{len(labels)})")
    print(f"  - ì˜¤ë¶„ë¥˜: {len(misclassified)}ê°œ")

    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
    print(f"\n{'='*80}")
    print("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ")
    print("="*80)

    # Precision, Recall, F1-score
    precision, recall, f1, support = precision_recall_fscore_support(
        labels, preds, labels=list(range(num_classes)), zero_division=0
    )

    print(f"{'í´ë˜ìŠ¤':<12} {'ì •í™•ë„':<10} {'Precision':<12} {'Recall':<10} {'F1-score':<12} {'í‰ê·  í™•ë¥ ':<10}")
    print("-" * 80)

    for i in range(num_classes):
        action = class_names[i]
        mask = labels == i
        class_acc = np.mean(preds[mask] == i) * 100 if mask.sum() > 0 else 0

        # í•´ë‹¹ í´ë˜ìŠ¤ì˜ í‰ê·  í™•ë¥ 
        action_upper = action.upper()
        if action_upper in class_probs and len(class_probs[action_upper]) > 0:
            avg_prob = np.mean([p[i] for p in class_probs[action_upper]]) * 100
        else:
            avg_prob = 0

        print(
            f"{action:<12} "
            f"{class_acc:>6.2f}%   "
            f"{precision[i]*100:>8.2f}%   "
            f"{recall[i]*100:>6.2f}%   "
            f"{f1[i]*100:>8.2f}%   "
            f"{avg_prob:>6.2f}%"
        )

    # Confusion Matrix
    print(f"\n{'='*80}")
    print("Confusion Matrix")
    print("="*80)

    cm = confusion_matrix(labels, preds, labels=list(range(num_classes)))

    # í—¤ë”
    header = "ì‹¤ì œ\\ì˜ˆì¸¡".ljust(12)
    for name in class_names:
        header += f"{name[:8]:>10}"
    print(header)
    print("-" * 80)

    # ê° í–‰
    for i, name in enumerate(class_names):
        row = f"{name[:12]:<12}"
        for j in range(num_classes):
            row += f"{cm[i, j]:>10}"
        print(row)

    # í™•ë¥  ë¶„í¬ ë¶„ì„
    print(f"\n{'='*80}")
    print("í™•ë¥  ë¶„í¬ ë¶„ì„")
    print("="*80)

    # ì ìˆ˜ ë³€í™˜ (inference.py ë¡œì§)
    score_90_plus = np.sum(target_probs >= 0.90)
    score_75_to_90 = np.sum((target_probs >= 0.75) & (target_probs < 0.90))
    score_60_to_75 = np.sum((target_probs >= 0.60) & (target_probs < 0.75))
    score_below_60 = np.sum(target_probs < 0.60)

    print(f"ëª©í‘œ í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬:")
    print(f"  - 90% ì´ìƒ (3ì ):  {score_90_plus:>4}ê°œ ({score_90_plus/len(labels)*100:>5.1f}%)")
    print(f"  - 75-90% (2ì ):    {score_75_to_90:>4}ê°œ ({score_75_to_90/len(labels)*100:>5.1f}%)")
    print(f"  - 60-75% (1ì ):    {score_60_to_75:>4}ê°œ ({score_60_to_75/len(labels)*100:>5.1f}%)")
    print(f"  - 60% ë¯¸ë§Œ (0ì ):  {score_below_60:>4}ê°œ ({score_below_60/len(labels)*100:>5.1f}%)")

    # ì ìˆ˜ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜
    scores = np.zeros_like(target_probs, dtype=int)
    scores[target_probs >= 0.90] = 3
    scores[(target_probs >= 0.75) & (target_probs < 0.90)] = 2
    scores[(target_probs >= 0.60) & (target_probs < 0.75)] = 1

    avg_score = np.mean(scores)
    avg_game_score = avg_score / 3 * 100  # 0-100ì  í™˜ì‚°

    print(f"\nâ–¶ ì ìˆ˜ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜ (inference.py ê¸°ì¤€):")
    print(f"  - í‰ê·  ì ìˆ˜: {avg_score:.2f}ì  (ê²Œì„ ì ìˆ˜: {avg_game_score:.1f}ì )")
    print(f"  - 3ì : {np.sum(scores == 3):>4}ê°œ ({np.sum(scores == 3)/len(scores)*100:>5.1f}%)")
    print(f"  - 2ì : {np.sum(scores == 2):>4}ê°œ ({np.sum(scores == 2)/len(scores)*100:>5.1f}%)")
    print(f"  - 1ì : {np.sum(scores == 1):>4}ê°œ ({np.sum(scores == 1)/len(scores)*100:>5.1f}%)")
    print(f"  - 0ì : {np.sum(scores == 0):>4}ê°œ ({np.sum(scores == 0)/len(scores)*100:>5.1f}%)")

    # STAY vs CLAP ë¶„ì„
    if "STAY" in class_probs and "CLAP" in class_probs:
        print(f"\n{'='*80}")
        print("STAY vs CLAP êµ¬ë¶„ ë¶„ì„ (ê°€ë§Œíˆ ìˆê¸° vs ì† ë°•ìˆ˜)")
        print("="*80)

        # CLAP, STAYì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        clap_idx = None
        stay_idx = None
        for idx, action in label_to_action.items():
            if action == "CLAP":
                clap_idx = idx
            elif action == "STAY":
                stay_idx = idx

        if clap_idx is not None and stay_idx is not None:
            # STAY ìƒ˜í”Œì˜ í™•ë¥  ë¶„ì„
            stay_probs = np.array(class_probs["STAY"])
            stay_stay_prob = stay_probs[:, stay_idx] * 100
            stay_clap_prob = stay_probs[:, clap_idx] * 100

            print(f"\nâ–¶ STAY ìƒ˜í”Œ ({len(stay_probs)}ê°œ):")
            print(f"  - STAY í™•ë¥  í‰ê· : {np.mean(stay_stay_prob):.2f}%")
            print(f"  - CLAP í™•ë¥  í‰ê· : {np.mean(stay_clap_prob):.2f}%")
            print(f"  - êµ¬ë¶„ ì„±ê³µë¥ : {np.sum(stay_stay_prob > stay_clap_prob) / len(stay_probs) * 100:.2f}%")

            # CLAP ìƒ˜í”Œì˜ í™•ë¥  ë¶„ì„
            clap_probs = np.array(class_probs["CLAP"])
            clap_clap_prob = clap_probs[:, clap_idx] * 100
            clap_stay_prob = clap_probs[:, stay_idx] * 100

            print(f"\nâ–¶ CLAP ìƒ˜í”Œ ({len(clap_probs)}ê°œ):")
            print(f"  - CLAP í™•ë¥  í‰ê· : {np.mean(clap_clap_prob):.2f}%")
            print(f"  - STAY í™•ë¥  í‰ê· : {np.mean(clap_stay_prob):.2f}%")
            print(f"  - êµ¬ë¶„ ì„±ê³µë¥ : {np.sum(clap_clap_prob > clap_stay_prob) / len(clap_probs) * 100:.2f}%")

            if np.mean(stay_stay_prob) > 95 and np.mean(clap_clap_prob) > 95:
                print(f"\n  âœ… ì™„ë²½í•œ êµ¬ë¶„! STAYì™€ CLAPë¥¼ í™•ì‹¤í•˜ê²Œ êµ¬ë³„í•©ë‹ˆë‹¤.")
            elif np.mean(stay_stay_prob) > 85 and np.mean(clap_clap_prob) > 85:
                print(f"\n  âœ… ìš°ìˆ˜í•œ êµ¬ë¶„! ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì •í™•í•˜ê²Œ êµ¬ë³„í•©ë‹ˆë‹¤.")
            else:
                print(f"\n  âš ï¸  ì£¼ì˜: STAYì™€ CLAP êµ¬ë¶„ì´ ì™„ë²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ìƒì„¸ ë¶„ì„
    if misclassified:
        print(f"\n{'='*80}")
        print(f"ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ìƒì„¸ ë¶„ì„ (ìƒìœ„ {min(10, len(misclassified))}ê°œ)")
        print("="*80)

        # í™•ë¥  ì°¨ì´ê°€ ì‘ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ì• ë§¤í•œ ì¼€ì´ìŠ¤)
        sorted_mis = sorted(misclassified, key=lambda x: abs(x["pred_prob"] - x["true_prob"]))

        for i, mis in enumerate(sorted_mis[:10], 1):
            prob_diff = mis["pred_prob"] - mis["true_prob"]

            print(f"\n{i}. ì‹¤ì œ: {mis['true_action']}, ì˜ˆì¸¡: {mis['pred_action']}")
            print(f"   - ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥ : {mis['true_prob']*100:.1f}%")
            print(f"   - ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥ : {mis['pred_prob']*100:.1f}%")
            print(f"   - í™•ë¥  ì°¨ì´: {prob_diff*100:+.1f}%p")
            print(f"   - íŒŒì¼: {mis['person']}/{mis['true_action']}/{mis['file'].name}")

            # ì ìˆ˜ ê³„ì‚°
            if mis["true_prob"] >= 0.90:
                score = 3
            elif mis["true_prob"] >= 0.75:
                score = 2
            elif mis["true_prob"] >= 0.60:
                score = 1
            else:
                score = 0
            print(f"   - ê²Œì„ ì ìˆ˜: {score}ì ")

            if abs(prob_diff) < 0.1:
                print(f"   - ë¶„ì„: ë§¤ìš° ì• ë§¤í•œ í™•ë¥  (ê±°ì˜ ë™ì )")
            elif abs(prob_diff) < 0.2:
                print(f"   - ë¶„ì„: ì• ë§¤í•œ í™•ë¥ ")
            else:
                print(f"   - ë¶„ì„: í™•ì‹ í•˜ê³  í‹€ë¦¼")

    print("\n" + "="*80)


def main():
    parser = argparse.ArgumentParser(description="í•™ìŠµëœ ëª¨ë¸ ì¢…í•© í‰ê°€")
    parser.add_argument("--model", type=str, default="checkpoints/gcn_cnn_best.pt", help="ëª¨ë¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--data_dir", type=str, default="./pose_sequences", help="ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--device", type=str, default="cuda", help="ë””ë°”ì´ìŠ¤ (cuda/cpu)")

    args = parser.parse_args()

    model_path = Path(args.model)
    data_dir = Path(args.data_dir)

    if not model_path.exists():
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    if not data_dir.exists():
        raise FileNotFoundError(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")

    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = args.device
    if device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        device = "cpu"

    print(f"â–¶ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"â–¶ ëª¨ë¸: {model_path}")
    print(f"â–¶ ë°ì´í„°: {data_dir}")

    # í´ë˜ìŠ¤ ë§¤í•‘
    action_to_label = {action: idx for idx, action in enumerate(sorted(SUPPORTED_ACTIONS))}
    label_to_action = {idx: action for action, idx in action_to_label.items()}
    num_classes = len(action_to_label)

    print(f"â–¶ í´ë˜ìŠ¤ ({num_classes}ê°œ): {', '.join(action_to_label.keys())}")

    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = load_model(model_path, num_classes, device)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    samples = load_all_samples(data_dir, action_to_label)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(samples)}ê°œ ìƒ˜í”Œ")

    # í‰ê°€
    results = evaluate_model_comprehensive(model, samples, label_to_action, device)

    # ê²°ê³¼ ì¶œë ¥
    print_evaluation_results(results, label_to_action)


if __name__ == "__main__":
    main()
