"""
MediaPipe í¬ì¦ˆ ëœë“œë§ˆí¬ ê¸°ë°˜ ë™ì‘ ì‹œí€€ìŠ¤ ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

data/ì´ë‹ˆì…œ/ë™ì‘/ í´ë”ì— ì €ì¥ëœ í”„ë ˆì„ ì´ë¯¸ì§€ë“¤ì„ ë¶ˆëŸ¬ì™€
ê° ì‹œí€€ìŠ¤(ì˜ˆ: clap_seq001_frame1~8)ì— ëŒ€í•œ í¬ì¦ˆ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•œ ë’¤
ì••ì¶•ëœ NumPy (.npz) íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python pose_sequence_extractor.py --data_dir ./data --output_dir ./pose_sequences
    python pose_sequence_extractor.py --data_dir ./data --persons JSY YHS --actions CLAP
"""

from __future__ import annotations

import argparse
from collections import defaultdict
import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import cv2
import mediapipe as mp
import numpy as np
from PIL import Image


SEQ_PATTERN = re.compile(
    r"(?P<base>.+)_seq(?P<seq>\d+)_frame(?P<frame>\d+)_backup\.(?P<ext>jpg|jpeg|png)$",
    re.IGNORECASE,
)


SUPPORTED_ACTIONS = ["CLAP", "ELBOW", "STRETCH", "TILT", "EXIT", "UNDERARM", "STAY"]  # 7ê°œ ë™ì‘


@dataclass
class SequenceResult:
    person: str
    action: str
    sequence_id: int
    frame_count: int
    saved_path: Path


def collect_sequences(
    action_dir: Path,
) -> Dict[int, List[Tuple[int, Path]]]:
    """
    action_dir ë‚´ ì´ë¯¸ì§€ íŒŒì¼ì„ ì‹œí€€ìŠ¤ID/í”„ë ˆì„IDë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
    """
    sequences: Dict[int, List[Tuple[int, Path]]] = defaultdict(list)
    for image_path in sorted(action_dir.glob("*")):
        if not image_path.is_file():
            continue
        match = SEQ_PATTERN.match(image_path.name)
        if not match:
            continue

        seq_id = int(match.group("seq"))
        frame_id = int(match.group("frame"))
        sequences[seq_id].append((frame_id, image_path))

    # í”„ë ˆì„ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
    for seq_id in list(sequences.keys()):
        sequences[seq_id] = sorted(sequences[seq_id], key=lambda item: item[0])

    return sequences


def extract_landmarks_from_image(
    pose: mp.solutions.pose.Pose,
    image_path: Path,
) -> Optional[np.ndarray]:
    # ========================================================================
    # âš ï¸ CRITICAL: Apply EXIF orientation correction
    # ========================================================================
    # Some images may have EXIF orientation metadata (rotation info)
    # cv2.imread() ignores EXIF, causing rotated images to be processed incorrectly
    # Solution: Use PIL to auto-rotate based on EXIF, then convert to cv2 format
    # ========================================================================
    with Image.open(image_path) as pil_img:
        # Auto-rotate based on EXIF orientation tag
        from PIL import ImageOps
        pil_img = ImageOps.exif_transpose(pil_img)
        if pil_img is None:
            # If exif_transpose returns None, reload original
            pil_img = Image.open(image_path)

        # Convert PIL (RGB) to OpenCV (BGR) format
        image_rgb = np.array(pil_img.convert("RGB"))
        image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)

    if image is None:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = pose.process(image_rgb)

    if not results.pose_landmarks:
        print(f"âš ï¸  í¬ì¦ˆë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {image_path}")
        return None

    landmarks = np.array(
        [
            [lm.x, lm.y]
            for lm in results.pose_landmarks.landmark
        ],
        dtype=np.float32,
    )
    return landmarks


def save_sequence(
    output_dir: Path,
    person: str,
    action: str,
    sequence_id: int,
    landmarks: np.ndarray,
    metadata: Dict[str, object],
    overwrite: bool = False,
) -> Path:
    person = person.upper()
    action = action.upper()
    output_dir = output_dir / person / action
    output_dir.mkdir(parents=True, exist_ok=True)

    filename = f"{action.lower()}_seq{sequence_id:03d}.npz"
    output_path = output_dir / filename

    if output_path.exists() and not overwrite:
        raise FileExistsError(f"ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤ (ë®ì–´ì“°ê¸° ë¹„í™œì„±í™”): {output_path}")

    np.savez_compressed(output_path, landmarks=landmarks, metadata=json.dumps(metadata))
    return output_path


def extract_pose_sequences(
    data_dir: Path,
    output_dir: Path,
    frames_per_sample: int,
    persons: Optional[Iterable[str]] = None,
    actions: Optional[Iterable[str]] = None,
    overwrite: bool = False,
    model_complexity: int = 1,
    min_detection_confidence: float = 0.5,
) -> List[SequenceResult]:
    """
    data_dir êµ¬ì¡°ë¥¼ ìˆœíšŒí•˜ë©° í¬ì¦ˆ ì‹œí€€ìŠ¤ë¥¼ ì¶”ì¶œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    data_dir = Path(data_dir)
    output_dir = Path(output_dir)

    if not data_dir.exists():
        raise FileNotFoundError(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")

    person_filter = {p.upper() for p in persons} if persons else None
    action_filter = {a.upper() for a in actions} if actions else None

    mp_pose = mp.solutions.pose
    results: List[SequenceResult] = []

    with mp_pose.Pose(
        static_image_mode=True,
        model_complexity=model_complexity,
        enable_segmentation=False,
        min_detection_confidence=min_detection_confidence,
    ) as pose:
        for person_dir in sorted(p for p in data_dir.iterdir() if p.is_dir()):
            person_name = person_dir.name.upper()
            if person_filter and person_name not in person_filter:
                continue

            for action_dir in sorted(p for p in person_dir.iterdir() if p.is_dir()):
                action_name = action_dir.name.upper()
                if action_filter and action_name not in action_filter:
                    continue
                if action_name not in SUPPORTED_ACTIONS:
                    print(f"âš ï¸  ì§€ì›ë˜ì§€ ì•ŠëŠ” ë™ì‘ìœ¼ë¡œ ê±´ë„ˆëœ€: {action_name}")
                    continue

                sequences = collect_sequences(action_dir)
                if not sequences:
                    print(f"âš ï¸  ì‹œí€€ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {action_dir}")
                    continue

                for sequence_id, frames in sequences.items():
                    if len(frames) != frames_per_sample:
                        print(
                            f"âš ï¸  í”„ë ˆì„ ìˆ˜ ë¶ˆì¼ì¹˜ë¡œ ê±´ë„ˆëœ€: "
                            f"{person_name}/{action_name} seq{sequence_id:03d} "
                            f"({len(frames)} í”„ë ˆì„, ê¸°ëŒ€: {frames_per_sample})"
                        )
                        continue

                    frame_landmarks: List[np.ndarray] = []
                    skip_sequence = False

                    for frame_idx, image_path in frames:
                        landmarks = extract_landmarks_from_image(pose, image_path)
                        if landmarks is None:
                            skip_sequence = True
                            break
                        frame_landmarks.append(landmarks)

                    if skip_sequence:
                        print(
                            f"âš ï¸  í¬ì¦ˆ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì‹œí€€ìŠ¤ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: "
                            f"{person_name}/{action_name} seq{sequence_id:03d}"
                        )
                        continue

                    landmarks_array = np.stack(frame_landmarks, axis=0)
                    metadata = {
                        "person": person_name,
                        "action": action_name,
                        "sequence_id": sequence_id,
                        "frames_per_sample": frames_per_sample,
                        "landmark_count": landmarks_array.shape[1],
                    }
                    try:
                        saved_path = save_sequence(
                            output_dir=output_dir,
                            person=person_name,
                            action=action_name,
                            sequence_id=sequence_id,
                            landmarks=landmarks_array,
                            metadata=metadata,
                            overwrite=overwrite,
                        )
                        results.append(
                            SequenceResult(
                                person=person_name,
                                action=action_name,
                                sequence_id=sequence_id,
                                frame_count=frames_per_sample,
                                saved_path=saved_path,
                            )
                        )
                        print(
                            f"âœ“ ì €ì¥ ì™„ë£Œ: {person_name}/{action_name} "
                            f"seq{sequence_id:03d} â†’ {saved_path}"
                        )
                    except FileExistsError as error:
                        print(f"âš ï¸  {error}")

    return results


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="MediaPipeë¥¼ ì´ìš©í•œ í¬ì¦ˆ ì‹œí€€ìŠ¤ (.npz) ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸",
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        required=True,
        help="í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ ì…ë ¥ ë””ë ‰í† ë¦¬ (ì˜ˆ: ./data)",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="pose_sequences",
        help="í¬ì¦ˆ ì‹œí€€ìŠ¤ë¥¼ ì €ì¥í•  ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./pose_sequences)",
    )
    parser.add_argument(
        "--frames_per_sample",
        type=int,
        default=8,
        help="ì‹œí€€ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸: 8)",
    )
    parser.add_argument(
        "--persons",
        nargs="*",
        default=None,
        help="íŠ¹ì • ì°¸ê°€ì(ì´ë‹ˆì…œ)ë§Œ ì²˜ë¦¬ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)",
    )
    parser.add_argument(
        "--actions",
        nargs="*",
        default=None,
        help=f"íŠ¹ì • ë™ì‘ë§Œ ì²˜ë¦¬ (ì˜ˆ: {' '.join(SUPPORTED_ACTIONS)})",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¶œë ¥ íŒŒì¼ì„ ë®ì–´ì“°ê¸°",
    )
    parser.add_argument(
        "--model_complexity",
        type=int,
        default=1,
        choices=[0, 1, 2],
        help="MediaPipe Pose ëª¨ë¸ ë³µì¡ë„ (0, 1, 2)",
    )
    parser.add_argument(
        "--min_detection_confidence",
        type=float,
        default=0.5,
        help="í¬ì¦ˆ ê°ì§€ë¥¼ ìœ„í•œ ìµœì†Œ ì‹ ë¢°ë„ (0.0~1.0)",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    results = extract_pose_sequences(
        data_dir=Path(args.data_dir),
        output_dir=Path(args.output_dir),
        frames_per_sample=args.frames_per_sample,
        persons=args.persons,
        actions=args.actions,
        overwrite=args.overwrite,
        model_complexity=args.model_complexity,
        min_detection_confidence=args.min_detection_confidence,
    )

    if results:
        summary = defaultdict(lambda: defaultdict(int))
        for result in results:
            summary[result.person][result.action] += 1

        print("\nğŸ“Š ì¶”ì¶œ ìš”ì•½")
        for person, actions in sorted(summary.items()):
            print(f"[{person}]")
            for action, count in sorted(actions.items()):
                print(f"  - {action}: {count}ê°œ")
    else:
        print("âš ï¸  ì €ì¥ëœ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

