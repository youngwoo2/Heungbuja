"""Brandnew ëª¨ì…˜ ì¶”ë¡  ì„œë¹„ìŠ¤ - ë…ë¦½ì ì¸ êµ¬í˜„ (test_server_simulation.py ê¸°ë°˜)"""

from __future__ import annotations

import base64
import logging
import os
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from time import perf_counter
from typing import List, Sequence

import cv2
import mediapipe as mp
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

LOGGER = logging.getLogger(__name__)


@dataclass
class InferenceResult:
    predicted_label: str
    confidence: float
    judgment: int
    decode_time_ms: float
    pose_time_ms: float
    inference_time_ms: float
    action_code: int | None
    target_probability: float | None = None


# ============================================================================
# ëª¨ë¸ êµ¬ì¡° ì •ì˜ (test_server_simulation.pyì™€ ì™„ì „íˆ ë™ì¼)
# ============================================================================

class GCNLayer(nn.Module):
    """GCN ë ˆì´ì–´ - adjacencyë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (softmax ì—†ìŒ!)"""

    def __init__(self, in_features: int, out_features: int, adjacency: torch.Tensor, dropout: float = 0.0):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(out_features)
        # âœ… adjacencyë¥¼ bufferë¡œ ë“±ë¡ (í•™ìŠµ ì•ˆ ë¨, checkpointì—ì„œ ë¡œë“œëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        self.register_buffer("adjacency", adjacency)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # âœ… softmax ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©!
        agg = torch.einsum("ij,btnf->btif", self.adjacency, x)
        out = self.linear(agg)
        out = self.dropout(out)
        out = self.norm(out)
        return out


class TemporalCNN(nn.Module):
    """ì‹œê³„ì—´ CNN ë¸”ë¡"""

    def __init__(self, in_channels: int, hidden_channels: Sequence[int], kernel_size: int = 3, dropout: float = 0.2):
        super().__init__()
        layers: List[nn.Module] = []
        prev = in_channels
        padding = kernel_size // 2
        for channels in hidden_channels:
            layers.extend([
                nn.Conv1d(prev, channels, kernel_size=kernel_size, padding=padding),
                nn.BatchNorm1d(channels),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
            ])
            prev = channels
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self.network(x)
        return out.mean(dim=-1)


class GCNTemporalModel(nn.Module):
    """GCN + Temporal CNN ëª¨ë¸ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)"""

    def __init__(self, input_dim: int, num_classes: int, adjacency: torch.Tensor,
                 gcn_hidden_dims: Sequence[int] = (64, 128),
                 temporal_channels: Sequence[int] = (128, 256),
                 dropout: float = 0.3) -> None:
        super().__init__()
        self.gcn_layers = nn.ModuleList()
        prev_dim = input_dim
        for hidden_dim in gcn_hidden_dims:
            self.gcn_layers.append(GCNLayer(prev_dim, hidden_dim, adjacency, dropout=dropout))
            prev_dim = hidden_dim

        self.temporal_cnn = TemporalCNN(prev_dim, temporal_channels, dropout=dropout)
        temporal_out_dim = temporal_channels[-1] if temporal_channels else prev_dim

        self.classifier = nn.Sequential(
            nn.Linear(temporal_out_dim, temporal_out_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(temporal_out_dim, num_classes),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for gcn in self.gcn_layers:
            x = F.relu(gcn(x))
        x = x.mean(dim=2)
        x = x.permute(0, 2, 1)
        features = self.temporal_cnn(x)
        logits = self.classifier(features)
        return logits


# ============================================================================
# MediaPipe Pose ì¶”ì¶œê¸°
# ============================================================================

class PoseExtractor:
    """MediaPipe Pose ì¶”ì¶œ"""

    def __init__(self) -> None:
        self._pose = mp.solutions.pose.Pose(
            static_image_mode=True,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
        )


# ============================================================================
# Brandnew ì¶”ë¡  ì„œë¹„ìŠ¤
# ============================================================================

class BrandnewMotionInferenceService:
    """Brandnew ëª¨ë¸ ì „ìš© ì¶”ë¡  ì„œë¹„ìŠ¤ - ë…ë¦½ì ì¸ êµ¬í˜„"""

    def __init__(self, model_path: Path, device: str | None = None) -> None:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
        args = checkpoint.get("args", {})
        class_mapping = checkpoint.get("class_mapping", {})

        if device:
            requested = torch.device(device)
            if requested.type == "cuda" and not torch.cuda.is_available():
                raise RuntimeError("CUDA ì¥ì¹˜ê°€ ìš”ì²­ë˜ì—ˆì§€ë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.device = requested
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        LOGGER.info("Brandnew model inference device: %s", self.device)
        self.frames_per_sample = int(args.get("frames_per_sample", 8))
        self.class_mapping = {label.upper(): index for label, index in class_mapping.items()}
        self.id_to_label = {index: label for label, index in self.class_mapping.items()}

        LOGGER.info("Brandnew model class mapping: %s", self.id_to_label)

        # ëª¨ë¸ íŒŒë¼ë¯¸í„°
        gcn_hidden_dims = args.get("gcn_hidden_dims", [64, 128])
        temporal_channels = args.get("temporal_channels", [128, 256])
        dropout = float(args.get("dropout", 0.3))
        adjacency = checkpoint["model_state_dict"]["gcn_layers.0.adjacency"]

        # âœ… ë…ë¦½ì ì¸ ëª¨ë¸ êµ¬ì¡° ì‚¬ìš©!
        self.model = GCNTemporalModel(
            input_dim=checkpoint["model_state_dict"]["gcn_layers.0.linear.weight"].shape[1],
            num_classes=len(class_mapping),
            adjacency=adjacency,
            gcn_hidden_dims=gcn_hidden_dims,
            temporal_channels=temporal_channels,
            dropout=dropout,
        )
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.model.to(self.device)
        self.model.eval()

        self.pose_extractor = PoseExtractor()

        # DB actionCode â†’ Model class_index ë§¤í•‘
        # ì‹¤ì œ ëª¨ë¸: 0: CLAP, 1: ELBOW, 2: EXIT, 3: STAY, 4: STRETCH, 5: TILT, 6: UNDERARM
        self.ACTION_CODE_TO_CLASS_INDEX = {
            1: self.class_mapping.get("CLAP"),       # ì† ë°•ìˆ˜ â†’ CLAP (ì¶”ê°€!)
            2: self.class_mapping.get("ELBOW"),      # íŒ” ì¹˜ê¸° â†’ ELBOW
            4: self.class_mapping.get("STRETCH"),    # íŒ” ë»—ê¸° â†’ STRETCH
            5: self.class_mapping.get("TILT"),       # ê¸°ìš°ëš± â†’ TILT
            6: self.class_mapping.get("EXIT"),       # ë¹„ìƒêµ¬ â†’ EXIT
            7: self.class_mapping.get("UNDERARM"),   # ê²¨ë“œë‘ì´ë°•ìˆ˜ â†’ UNDERARM
            9: self.class_mapping.get("STAY"),       # ê°€ë§Œíˆ ìˆìŒ â†’ STAY
        }

        # Model class_index â†’ DB actionCode (ì—­ë§¤í•‘)
        self.CLASS_INDEX_TO_ACTION_CODE = {}
        for action_code, class_idx in self.ACTION_CODE_TO_CLASS_INDEX.items():
            if class_idx is not None:
                self.CLASS_INDEX_TO_ACTION_CODE[class_idx] = action_code

        LOGGER.info("Brandnew ACTION_CODE mapping: %s", self.ACTION_CODE_TO_CLASS_INDEX)

    def predict_from_poses(
        self,
        pose_frames: Sequence[Sequence[Sequence[float]]],
        target_action_name: str | None = None,
        target_action_code: int | None = None,
    ) -> InferenceResult:
        """
        Pose ì¢Œí‘œ ì‹œí€€ìŠ¤ë¥¼ ì§ì ‘ ë°›ì•„ ë™ì‘ ì˜ˆì¸¡ ìˆ˜í–‰ (ìƒˆë¡œìš´ ë°©ì‹ - MediaPipe ìŠ¤í‚µ)

        Args:
            pose_frames: í”„ë ˆì„ë³„ ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸. ê° í”„ë ˆì„ì€ 33ê°œ ëœë“œë§ˆí¬ì˜ [x, y] ì¢Œí‘œ
                        í˜•íƒœ: [[[x0, y0], [x1, y1], ...], ...]  (frames, 33, 2)
            target_action_name: ëª©í‘œ ë™ì‘ ì´ë¦„ (ì„ íƒ)
            target_action_code: ëª©í‘œ ë™ì‘ ì½”ë“œ (ì„ íƒ)
        """
        from time import perf_counter

        if not pose_frames:
            raise ValueError("Pose ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        start_time = perf_counter()

        # í”„ë ˆì„ ìƒ˜í”Œë§
        sampled_frames = self._sample_pose_frames(pose_frames, self.frames_per_sample)

        # numpy ë°°ì—´ë¡œ ë³€í™˜: (T, 33, 2)
        raw_sequence = np.array(sampled_frames, dtype=np.float32)

        if raw_sequence.shape[1] != 33 or raw_sequence.shape[2] != 2:
            raise ValueError(
                f"ì˜ëª»ëœ ì¢Œí‘œ í˜•ì‹ì…ë‹ˆë‹¤. ê¸°ëŒ€: (T, 33, 2), ì‹¤ì œ: {raw_sequence.shape}"
            )

        # ì •ê·œí™” (ê¸°ì¡´ê³¼ ë™ì¼í•œ ë°©ì‹)
        normalized_sequence = self._normalize_sequence(raw_sequence)
        preprocess_time_ms = (perf_counter() - start_time) * 1000

        LOGGER.info("ğŸ” Pose ì…ë ¥ - shape: %s", normalized_sequence.shape)

        # ëª¨ë¸ ì¶”ë¡ 
        input_tensor = torch.from_numpy(normalized_sequence).unsqueeze(0).to(self.device)

        with torch.no_grad():
            inference_start = perf_counter()
            logits = self.model(input_tensor)
            inference_time_ms = (perf_counter() - inference_start) * 1000
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

        best_idx = int(np.argmax(probabilities))
        predicted_label = self.id_to_label.get(best_idx, "UNKNOWN")
        confidence = float(probabilities[best_idx])

        target_index = self._resolve_target_index(target_action_name, target_action_code)
        target_probability: float | None = None
        if target_index is not None and 0 <= target_index < len(probabilities):
            target_probability = float(probabilities[target_index])
            judgment = self._score_by_probability(target_probability)
        else:
            judgment = self._fallback_score(predicted_label, confidence, target_action_name)

        LOGGER.info(
            "ğŸ¯ Pose AI íŒì • - ëª©í‘œ=%s(code=%s), ì˜ˆì¸¡=%s(%.1f%%), ì ìˆ˜=%dì ",
            target_action_name, target_action_code, predicted_label, confidence * 100, judgment
        )

        if target_action_code is not None:
            resolved_action_code = target_action_code
        else:
            resolved_action_code = self.CLASS_INDEX_TO_ACTION_CODE.get(best_idx, best_idx + 1)

        return InferenceResult(
            predicted_label=predicted_label,
            confidence=confidence,
            judgment=judgment,
            action_code=resolved_action_code,
            decode_time_ms=0.0,  # ì´ë¯¸ì§€ ë””ì½”ë”© ì—†ìŒ
            pose_time_ms=preprocess_time_ms,  # ì „ì²˜ë¦¬ ì‹œê°„
            inference_time_ms=inference_time_ms,
            target_probability=target_probability,
        )

    def _sample_pose_frames(
        self, frames: Sequence[Sequence[Sequence[float]]], target_count: int
    ) -> list:
        """Pose í”„ë ˆì„ ìƒ˜í”Œë§"""
        if len(frames) == target_count:
            return list(frames)

        if len(frames) < target_count:
            padding = [frames[-1]] * (target_count - len(frames))
            return list(frames) + padding

        indices = np.linspace(0, len(frames) - 1, target_count).astype(int)
        return [frames[i] for i in indices]

    def predict(
        self,
        frames: Sequence[str],
        target_action_name: str | None = None,
        target_action_code: int | None = None,
    ) -> InferenceResult:
        """í”„ë ˆì„ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ ë™ì‘ ì˜ˆì¸¡ ìˆ˜í–‰"""
        if not frames:
            raise ValueError("í”„ë ˆì„ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        # ì „ì²˜ë¦¬: í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ ì •ê·œí™”
        sampled_frames = self._sample_frames(frames, self.frames_per_sample)

        # ë””ë²„ê¹…: í”„ë ˆì„ ì €ì¥
        self._save_frames_for_debug(sampled_frames, target_action_name, target_action_code)

        keypoint_sequence, decode_time_s, pose_time_s = self._frames_to_keypoints(sampled_frames)

        LOGGER.info("ğŸ” Brandnew - Keypoint sequence shape: %s", keypoint_sequence.shape)

        input_tensor = torch.from_numpy(keypoint_sequence).unsqueeze(0)
        input_tensor = input_tensor.to(self.device)

        with torch.no_grad():
            inference_start = perf_counter()
            logits = self.model(input_tensor)
            inference_time_ms = (perf_counter() - inference_start) * 1000
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

            LOGGER.info("ğŸ” Brandnew - Logits: %s", logits.cpu().numpy()[0])
            LOGGER.info("ğŸ” Brandnew - Probabilities: %s", probabilities)

        decode_time_ms = decode_time_s * 1000
        pose_time_ms = pose_time_s * 1000

        best_idx = int(np.argmax(probabilities))
        predicted_label = self.id_to_label.get(best_idx, "UNKNOWN")
        confidence = float(probabilities[best_idx])

        target_index = self._resolve_target_index(target_action_name, target_action_code)
        target_probability: float | None = None
        if target_index is not None and 0 <= target_index < len(probabilities):
            target_probability = float(probabilities[target_index])
            judgment = self._score_by_probability(target_probability)
        else:
            judgment = self._fallback_score(predicted_label, confidence, target_action_name)

        total_time_ms = decode_time_ms + pose_time_ms + inference_time_ms
        LOGGER.info(
            "ğŸ¯ Brandnew AI íŒì • - ëª©í‘œ=%s(code=%s), ì˜ˆì¸¡=%s(%.1f%%), "
            "ëª©í‘œí™•ë¥ =%.1f%%, ì ìˆ˜=%dì  | â±ï¸ ì´=%.0fms",
            target_action_name,
            target_action_code,
            predicted_label,
            confidence * 100,
            (target_probability * 100) if target_probability else 0,
            judgment,
            total_time_ms,
        )

        # actionCode ë³€í™˜
        if target_action_code is not None:
            resolved_action_code = target_action_code
        else:
            resolved_action_code = self.CLASS_INDEX_TO_ACTION_CODE.get(best_idx, best_idx + 1)

        return InferenceResult(
            predicted_label=predicted_label,
            confidence=confidence,
            judgment=judgment,
            action_code=resolved_action_code,
            decode_time_ms=decode_time_ms,
            pose_time_ms=pose_time_ms,
            inference_time_ms=inference_time_ms,
            target_probability=target_probability,
        )

    def _resolve_target_index(
        self, action_name: str | None, action_code: int | None
    ) -> int | None:
        """ëª©í‘œ ë™ì‘ì„ ëª¨ë¸ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
        if action_code is not None:
            model_index = self.ACTION_CODE_TO_CLASS_INDEX.get(action_code)
            if model_index is not None and model_index in self.id_to_label:
                return model_index

        if action_name:
            key = action_name.strip().upper()
            return self.class_mapping.get(key)

        return None

    @staticmethod
    def _score_by_probability(probability: float) -> int:
        """í™•ë¥  ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€)"""
        if probability >= 0.60:  # 70 â†’ 60
            return 3
        if probability >= 0.40:  # 50 â†’ 40
            return 2
        if probability >= 0.25:  # 30 â†’ 25 (ELBOW 30% ëŒ€ì‘)
            return 1
        return 0

    def _fallback_score(
        self, predicted_label: str, confidence: float, target_action: str | None
    ) -> int:
        """ëª©í‘œ í™•ë¥ ì´ ì—†ì„ ë•Œ í´ë°± ì ìˆ˜ ê³„ì‚° (ë§¤ìš° ì™„í™”ëœ ê¸°ì¤€)"""
        if not target_action:
            if confidence >= 0.60:  # 70 â†’ 60
                return 3
            if confidence >= 0.40:  # 50 â†’ 40
                return 2
            if confidence >= 0.25:  # 30 â†’ 25
                return 1
            return 0

        target_key = target_action.strip().upper()
        predicted_key = predicted_label.strip().upper()

        if target_key == predicted_key:
            if confidence >= 0.60:  # 70 â†’ 60
                return 3
            if confidence >= 0.40:  # 50 â†’ 40
                return 2
            if confidence >= 0.25:  # 30 â†’ 25
                return 1
            return 0
        else:
            return 0

    def _frames_to_keypoints(self, frames: Sequence[str]):
        """í”„ë ˆì„ì„ í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ (test_server_simulation.pyì™€ ë™ì¼)"""
        raw_landmarks_list = []
        decode_elapsed = 0.0
        pose_elapsed = 0.0
        valid_count = 0
        total_count = 0

        for encoded in frames:
            total_count += 1

            # ì´ë¯¸ì§€ ë””ì½”ë”© (cv2 ë°©ì‹)
            decode_start = perf_counter()
            try:
                image_data = base64.b64decode(encoded)
            except Exception as exc:
                raise ValueError("Base64 ë””ì½”ë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") from exc

            nparr = np.frombuffer(image_data, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if image is None:
                raise ValueError("ì´ë¯¸ì§€ ë””ì½”ë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            decode_elapsed += perf_counter() - decode_start

            # Pose ì¶”ì¶œ
            pose_start = perf_counter()
            results = self.pose_extractor._pose.process(image_rgb)
            pose_elapsed += perf_counter() - pose_start

            if not results.pose_landmarks:
                continue

            landmarks = results.pose_landmarks.landmark
            all_coords = np.array([(lm.x, lm.y) for lm in landmarks], dtype=np.float32)
            raw_landmarks_list.append(all_coords)
            valid_count += 1

        # ìµœì†Œ í”„ë ˆì„ ì²´í¬
        MIN_VALID_FRAMES = 5
        if valid_count < MIN_VALID_FRAMES:
            raise ValueError(
                f"ìœ íš¨í•œ ë™ì‘ í”„ë ˆì„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ ({valid_count}/{total_count}ê°œ). "
                f"ì¹´ë©”ë¼ì— ì „ì‹ ì´ ë³´ì´ë„ë¡ í•´ì£¼ì„¸ìš”."
            )

        LOGGER.info("ğŸ“¹ Brandnew - í”„ë ˆì„ ë¶„ì„: ìœ íš¨=%dê°œ, ì „ì²´=%dê°œ", valid_count, total_count)

        # (T, 33, 2) í˜•íƒœë¡œ ìŠ¤íƒ
        raw_sequence = np.stack(raw_landmarks_list, axis=0)

        # ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì •ê·œí™”
        normalized_sequence = self._normalize_sequence(raw_sequence)

        return normalized_sequence, decode_elapsed, pose_elapsed

    @staticmethod
    def _normalize_sequence(landmarks_sequence: np.ndarray) -> np.ndarray:
        """
        ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ì •ê·œí™” (test_server_simulation.pyì™€ ì™„ì „íˆ ë™ì¼)

        Args:
            landmarks_sequence: (T, 33, 2) raw landmarks

        Returns:
            (T, 22, 2) normalized body keypoints
        """
        HIP_INDICES = (23, 24)
        USED_LANDMARK_INDICES = list(range(11, 33))

        coords = landmarks_sequence[..., :2]
        pelvis = (coords[:, HIP_INDICES[0], :] + coords[:, HIP_INDICES[1], :]) / 2.0
        coords = coords - pelvis[:, None, :]

        body_coords = coords[:, USED_LANDMARK_INDICES, :]
        max_range = np.max(np.linalg.norm(body_coords, axis=-1, ord=2))
        if max_range < 1e-6:
            max_range = 1.0
        body_coords = body_coords / max_range

        return body_coords.astype(np.float32)

    def _sample_frames(self, frames: Sequence[str], target_count: int):
        """í”„ë ˆì„ ìƒ˜í”Œë§"""
        if len(frames) == target_count:
            return list(frames)

        if len(frames) < target_count:
            padding = [frames[-1]] * (target_count - len(frames))
            return list(frames) + padding

        indices = np.linspace(0, len(frames) - 1, target_count).astype(int)
        return [frames[i] for i in indices]

    def _save_frames_for_debug(
        self,
        frames: Sequence[str],
        target_action_name: str | None,
        target_action_code: int | None,
    ):
        """ë””ë²„ê¹…ìš© í”„ë ˆì„ ì €ì¥"""
        import datetime
        from pathlib import Path

        # ì €ì¥ ë””ë ‰í† ë¦¬
        debug_dir = Path("/app/debug_frames")
        debug_dir.mkdir(exist_ok=True)

        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        action_info = f"{target_action_name or 'unknown'}_{target_action_code or 0}"

        # ê° í”„ë ˆì„ ì €ì¥
        for i, frame_b64 in enumerate(frames):
            try:
                # Base64 ë””ì½”ë”©
                frame_bytes = base64.b64decode(frame_b64)
                frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)
                frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)

                # íŒŒì¼ëª…: {timestamp}_{action}_{frame_num}.jpg
                filename = f"{timestamp}_{action_info}_frame{i:02d}.jpg"
                filepath = debug_dir / filename

                cv2.imwrite(str(filepath), frame)
            except Exception as e:
                LOGGER.warning("í”„ë ˆì„ %d ì €ì¥ ì‹¤íŒ¨: %s", i, e)

        LOGGER.info("ğŸ–¼ï¸ ë””ë²„ê·¸ í”„ë ˆì„ ì €ì¥ ì™„ë£Œ: %s (%dê°œ)", action_info, len(frames))


@lru_cache(maxsize=1)
def get_brandnew_inference_service() -> BrandnewMotionInferenceService:
    """Brandnew ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì¶”ë¡  ì„œë¹„ìŠ¤ ë°˜í™˜."""
    model_path = Path(__file__).resolve().parent.parent / "brandnewTrain" / "checkpoints" / "brandnew_model_v2.pt"

    if not model_path.exists():
        raise FileNotFoundError(f"Brandnew ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    device_override = os.getenv("MOTION_INFERENCE_DEVICE")
    LOGGER.info("Loading brandnew model from: %s", model_path)

    return BrandnewMotionInferenceService(model_path=model_path, device=device_override)
