"""Î™®ÏÖò Ï∂îÎ°† Î™®Îç∏ Î∞è Ï†ÑÏ≤òÎ¶¨ Ïú†Ìã∏Î¶¨Ìã∞."""

from __future__ import annotations

import base64
import logging
import os
from dataclasses import dataclass
from functools import lru_cache
from io import BytesIO
from pathlib import Path
from time import perf_counter
from typing import Iterable, List, Sequence, Tuple

import mediapipe as mp
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image

LOGGER = logging.getLogger(__name__)


@dataclass
class InferenceResult:
    predicted_label: str
    confidence: float
    judgment: int
    decode_time_ms: float
    pose_time_ms: float
    inference_time_ms: float
    action_code: int | None
    target_probability: float | None = None


class GraphConvLayer(nn.Module):
    """Í∞ÑÎã®Ìïú GCN Î†àÏù¥Ïñ¥."""

    def __init__(self, num_nodes: int, in_features: int, out_features: int) -> None:
        super().__init__()
        self.register_parameter(
            "adjacency", nn.Parameter(torch.eye(num_nodes, dtype=torch.float32))
        )
        self.linear = nn.Linear(in_features, out_features)
        self.norm = nn.LayerNorm(out_features)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, N, F)
        adjacency = torch.softmax(self.adjacency, dim=-1)
        aggregated = torch.einsum("nj,btjf->btnf", adjacency, x)
        out = self.linear(aggregated)
        out = self.norm(out)
        return F.relu(out)


class TemporalCNN(nn.Module):
    """ÏãúÍ≥ÑÏó¥ Ìå®ÌÑ¥ Ï∂îÏ∂úÏö© 1D CNN Î∏îÎ°ù."""

    def __init__(
        self,
        in_channels: int,
        channel_sizes: Sequence[int],
        dropout: float,
    ) -> None:
        super().__init__()
        layers: List[nn.Module] = []
        input_channels = in_channels
        for out_channels in channel_sizes:
            layers.append(nn.Conv1d(input_channels, out_channels, kernel_size=3, padding=1))
            layers.append(nn.BatchNorm1d(out_channels))
            layers.append(nn.ReLU(inplace=True))
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            input_channels = out_channels
        self.network = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


class MotionGCNCNN(nn.Module):
    """GCN + Temporal CNN Í∏∞Î∞ò Î™®ÏÖò Î∂ÑÎ•òÍ∏∞."""

    def __init__(
        self,
        num_nodes: int,
        input_dim: int,
        gcn_hidden_dims: Sequence[int],
        temporal_channels: Sequence[int],
        num_classes: int,
        dropout: float,
    ) -> None:
        super().__init__()
        self.gcn_layers = nn.ModuleList()
        prev_dim = input_dim
        for hidden_dim in gcn_hidden_dims:
            self.gcn_layers.append(GraphConvLayer(num_nodes, prev_dim, hidden_dim))
            prev_dim = hidden_dim

        self.temporal_cnn = TemporalCNN(prev_dim, temporal_channels, dropout)
        self.temporal_pool = nn.AdaptiveAvgPool1d(1)
        classifier_layers: List[nn.Module] = [
            nn.Linear(temporal_channels[-1], temporal_channels[-1]),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(temporal_channels[-1], num_classes),
        ]
        self.classifier = nn.Sequential(*classifier_layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, T, N, F)
        for layer in self.gcn_layers:
            x = layer(x)

        # ÎÖ∏Îìú ÌèâÍ∑†ÏúºÎ°ú Í∑∏ÎûòÌîÑ ÏûÑÎ≤†Îî© ÏÉùÏÑ±
        x = torch.mean(x, dim=2)  # (B, T, H)
        x = x.permute(0, 2, 1)  # (B, H, T)
        x = self.temporal_cnn(x)
        x = self.temporal_pool(x).squeeze(-1)
        logits = self.classifier(x)
        return logits


class PoseExtractor:
    """Mediapipe PoseÎ•º ÌôúÏö©Ìïú 2D Í¥ÄÏ†à Ï¢åÌëú Ï∂îÏ∂úÍ∏∞."""

    # ========================================================================
    # ‚ö†Ô∏è CRITICAL: Must match train_gcn_cnn.py preprocessing exactly!
    # ========================================================================
    # MediaPipe returns 33 landmarks (0-32)
    # train_gcn_cnn.py uses USED_LANDMARK_INDICES = list(range(11, 33))
    # This corresponds to landmarks 11-32 (22 landmarks, excluding face 0-10)
    # ========================================================================

    # MediaPipe HIP indices in full 33-landmark array
    # train_gcn_cnn.py: HIP_INDICES = (23, 24)
    FULL_LEFT_HIP_IDX = 23
    FULL_RIGHT_HIP_IDX = 24

    # Landmarks to extract (indices 11-32 from MediaPipe 33-landmark array)
    # This matches train_gcn_cnn.py's USED_LANDMARK_INDICES = list(range(11, 33))
    USED_LANDMARK_INDICES = list(range(11, 33))  # 22 landmarks

    def __init__(self) -> None:
        self._pose = mp.solutions.pose.Pose(
            static_image_mode=True,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.5,
        )

    def extract(self, image: np.ndarray) -> np.ndarray:
        """
        Extract and normalize keypoints using EXACT same method as training.

        Returns: (22, 2) normalized keypoints matching train_gcn_cnn.py preprocessing
        """
        results = self._pose.process(image)
        if not results.pose_landmarks:
            return np.zeros((len(self.USED_LANDMARK_INDICES), 2), dtype=np.float32)

        landmarks = results.pose_landmarks.landmark

        # ========================================================================
        # Step 1: Extract ALL 33 landmarks first (matching training preprocessing)
        # ========================================================================
        all_coords = np.array(
            [(lm.x, lm.y) for lm in landmarks],
            dtype=np.float32
        )  # (33, 2)

        # ========================================================================
        # Step 2: Center using pelvis (average of left/right hip) from FULL array
        # This matches train_gcn_cnn.py:75-76
        # pelvis = (coords[:, HIP_INDICES[0], :] + coords[:, HIP_INDICES[1], :]) / 2.0
        # coords = coords - pelvis[:, None, :]
        # ========================================================================
        pelvis = (all_coords[self.FULL_LEFT_HIP_IDX] + all_coords[self.FULL_RIGHT_HIP_IDX]) / 2.0
        all_coords = all_coords - pelvis  # (33, 2) centered

        # ========================================================================
        # Step 3: Select USED_LANDMARK_INDICES (11-32) - 22 landmarks
        # This matches train_gcn_cnn.py:78
        # body_coords = coords[:, USED_LANDMARK_INDICES, :]
        # ========================================================================
        body_coords = all_coords[self.USED_LANDMARK_INDICES]  # (22, 2)

        # ========================================================================
        # Step 4: Normalize by max norm of selected landmarks
        # This matches train_gcn_cnn.py:79-82
        # max_range = np.max(np.linalg.norm(body_coords, axis=-1, ord=2))
        # body_coords = body_coords / max_range
        # ========================================================================
        max_range = np.max(np.linalg.norm(body_coords, axis=-1, ord=2))
        if max_range < 1e-6:
            max_range = 1.0
        body_coords = body_coords / max_range

        return body_coords.astype(np.float32)


class MotionInferenceService:
    """ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Î°úÎìúÌïòÏó¨ ÌîÑÎ†àÏûÑ ÏãúÌÄÄÏä§Î•º ÌåêÏ†ï Ï†êÏàòÎ°ú Î≥ÄÌôò."""

    def __init__(self, model_path: Path, device: str | None = None) -> None:
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=False)
        args = checkpoint.get("args", {})
        class_mapping = checkpoint.get("class_mapping", {})

        if device:
            requested = torch.device(device)
            if requested.type == "cuda" and not torch.cuda.is_available():
                raise RuntimeError("CUDA Ïû•ÏπòÍ∞Ä ÏöîÏ≤≠ÎêòÏóàÏßÄÎßå ÏÇ¨Ïö© Í∞ÄÎä•ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")
            self.device = requested
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        LOGGER.info("Motion inference device: %s", self.device)
        self.frames_per_sample = int(args.get("frames_per_sample", 8))
        self.class_mapping = {label.upper(): index for label, index in class_mapping.items()}
        self.id_to_label = {index: label for label, index in self.class_mapping.items()}

        gcn_hidden_dims = args.get("gcn_hidden_dims", [64, 128])
        temporal_channels = args.get("temporal_channels", [128, 256])
        dropout = float(args.get("dropout", 0.3))

        self.model = MotionGCNCNN(
            num_nodes=checkpoint["model_state_dict"]["gcn_layers.0.adjacency"].shape[0],
            input_dim=checkpoint["model_state_dict"]["gcn_layers.0.linear.weight"].shape[1],
            gcn_hidden_dims=gcn_hidden_dims,
            temporal_channels=temporal_channels,
            num_classes=len(self.class_mapping),
            dropout=dropout,
        )
        self.model.load_state_dict(checkpoint["model_state_dict"])
        self.model.to(self.device)
        self.model.eval()

        self.pose_extractor = PoseExtractor()

    def predict(
        self,
        frames: Sequence[str],
        target_action_name: str | None = None,
        target_action_code: int | None = None,
    ) -> InferenceResult:
        if not frames:
            raise ValueError("ÌîÑÎ†àÏûÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.")

        sampled_frames = self._sample_frames(frames, self.frames_per_sample)
        keypoint_sequence, decode_time_s, pose_time_s = self._frames_to_keypoints(
            sampled_frames
        )

        # üîç ÎîîÎ≤ÑÍπÖ: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
        LOGGER.info("üîç ÎîîÎ≤ÑÍπÖ - Keypoint sequence shape: %s", keypoint_sequence.shape)
        LOGGER.info("üîç ÎîîÎ≤ÑÍπÖ - Keypoint stats - mean: %.4f, std: %.4f, min: %.4f, max: %.4f",
                   keypoint_sequence.mean(), keypoint_sequence.std(),
                   keypoint_sequence.min(), keypoint_sequence.max())

        input_tensor = torch.from_numpy(keypoint_sequence).unsqueeze(0)  # (1, T, N, 2)
        input_tensor = input_tensor.to(self.device)

        with torch.no_grad():
            inference_start = perf_counter()
            logits = self.model(input_tensor)
            inference_time_ms = (perf_counter() - inference_start) * 1000
            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

            # üîç ÎîîÎ≤ÑÍπÖ: Î™®Îç∏ Ï∂úÎ†• ÌôïÏù∏
            LOGGER.info("üîç ÎîîÎ≤ÑÍπÖ - Logits: %s", logits.cpu().numpy()[0])
            LOGGER.info("üîç ÎîîÎ≤ÑÍπÖ - Probabilities: %s", probabilities)
            LOGGER.info("üîç ÎîîÎ≤ÑÍπÖ - Class mapping: %s", self.id_to_label)

        decode_time_ms = decode_time_s * 1000
        pose_time_ms = pose_time_s * 1000

        best_idx = int(np.argmax(probabilities))
        predicted_label = self.id_to_label.get(best_idx, "UNKNOWN")
        confidence = float(probabilities[best_idx])

        target_index = self._resolve_target_index(target_action_name, target_action_code)
        target_probability: float | None = None
        if target_index is not None and 0 <= target_index < len(probabilities):
            target_probability = float(probabilities[target_index])
            judgment = self._score_by_probability(target_probability)
        else:
            judgment = self._fallback_score(predicted_label, confidence, target_action_name)

        total_time_ms = decode_time_ms + pose_time_ms + inference_time_ms
        LOGGER.info(
            "üéØ AI ÌåêÏ†ï Í≤∞Í≥º - Î™©ÌëúÎèôÏûë=%s(code=%s), ÏòàÏ∏°ÎèôÏûë=%s(Ïã†Î¢∞ÎèÑ=%.1f%%), "
            "Î™©ÌëúÌôïÎ•†=%.1f%%, Ï†êÏàò=%dÏ†ê | ‚è±Ô∏è Ï¥ù=%.0fms (ÎîîÏΩîÎî©=%.0fms, PoseÏ∂îÏ∂ú=%.0fms, Ï∂îÎ°†=%.0fms)",
            target_action_name,
            target_action_code,
            predicted_label,
            confidence * 100,
            (target_probability * 100) if target_probability else 0,
            judgment,
            total_time_ms,
            decode_time_ms,
            pose_time_ms,
            inference_time_ms,
        )

        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: Convert model index back to DB actionCode when returning
        # ========================================================================
        # Model class_index ‚Üí DB actionCode Ïó≠Îß§Ìïë
        CLASS_INDEX_TO_ACTION_CODE = {
            0: 1,  # CLAP ‚Üí ÏÜê Î∞ïÏàò
            1: 2,  # ELBOW ‚Üí Ìåî ÏπòÍ∏∞
            2: 4,  # STRETCH ‚Üí Ìåî ÎªóÍ∏∞
            3: 5,  # TILT ‚Üí Í∏∞Ïö∞Îö±
            4: 6,  # EXIT ‚Üí ÎπÑÏÉÅÍµ¨
            5: 7,  # UNDERARM ‚Üí Í≤®ÎìúÎûëÏù¥Î∞ïÏàò
            6: 9,  # STAY ‚Üí Í∞ÄÎßåÌûà ÏûàÏùå
        }
        # ========================================================================
        if target_action_code is not None:
            resolved_action_code = target_action_code
        else:
            # Î™®Îç∏Ïùò best_idxÎ•º Ïò¨Î∞îÎ•∏ actionCodeÎ°ú Î≥ÄÌôò
            resolved_action_code = CLASS_INDEX_TO_ACTION_CODE.get(best_idx, best_idx + 1)

        return InferenceResult(
            predicted_label=predicted_label,
            confidence=confidence,
            judgment=judgment,
            action_code=resolved_action_code,
            decode_time_ms=decode_time_ms,
            pose_time_ms=pose_time_ms,
            inference_time_ms=inference_time_ms,
            target_probability=target_probability,
        )

    def _resolve_target_index(
        self, action_name: str | None, action_code: int | None
    ) -> int | None:
        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: DB actionCode ‚Üí Model class_index Mapping
        # ========================================================================
        # DBÏùò actionCodeÏôÄ Î™®Îç∏Ïùò class_indexÎäî 1:1 Îß§ÌïëÏù¥ ÏïÑÎãôÎãàÎã§!
        # ÏùºÎ∂Ä DB ÎèôÏûëÏùÄ Î™®Îç∏Ïóê ÌïôÏäµÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.
        #
        # ModelÏóê ÌïôÏäµÎêú ÎèôÏûë (7Í∞ú):
        #   class 0: CLAP (ÏÜê Î∞ïÏàò)
        #   class 1: ELBOW (Ìåî ÏπòÍ∏∞)
        #   class 2: STRETCH (Ìåî ÎªóÍ∏∞)
        #   class 3: TILT (Í∏∞Ïö∞Îö±)
        #   class 4: EXIT (ÎπÑÏÉÅÍµ¨)
        #   class 5: UNDERARM (Í≤®ÎìúÎûëÏù¥ Î∞ïÏàò)
        #   class 6: STAY (Í∞ÄÎßåÌûà)
        #
        # DB actionCode ‚Üí Model class_index Îß§Ìïë:
        ACTION_CODE_TO_CLASS_INDEX = {
            1: 0,  # ÏÜê Î∞ïÏàò ‚Üí CLAP
            2: 1,  # Ìåî ÏπòÍ∏∞ ‚Üí ELBOW
            # 3: None,  # ÏóâÎç©Ïù¥ Î∞ïÏàò (Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå)
            4: 2,  # Ìåî ÎªóÍ∏∞ ‚Üí STRETCH
            5: 3,  # Í∏∞Ïö∞Îö± ‚Üí TILT
            6: 4,  # ÎπÑÏÉÅÍµ¨ ‚Üí EXIT
            7: 5,  # Í≤®ÎìúÎûëÏù¥Î∞ïÏàò ‚Üí UNDERARM
            # 8: None,  # Ìåî Î™®ÏúºÍ∏∞ (ÌïôÏäµ Ïïà Ìï®)
            9: 6,  # Í∞ÄÎßåÌûà ÏûàÏùå ‚Üí STAY
        }
        # ========================================================================

        if action_code is not None:
            # DB actionCodeÎ•º Model class_indexÎ°ú Î≥ÄÌôò
            model_index = ACTION_CODE_TO_CLASS_INDEX.get(action_code)
            if model_index is not None and model_index in self.id_to_label:
                return model_index

        if action_name:
            key = action_name.strip().upper()
            return self.class_mapping.get(key)

        return None

    @staticmethod
    def _score_by_probability(probability: float) -> int:
        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: Stricter scoring criteria to prevent high scores for
        # incorrect or minimal movements
        # ========================================================================
        # Previous issue: 51% threshold was too lenient
        # - Even when stationary, model could predict 51-60% ‚Üí judgment=2 (66.7Ï†ê)
        # - Resulted in unfair scores: moving=75Ï†ê vs stationary=73Ï†ê
        #
        # New stricter criteria:
        # - 90%+ ‚Üí 3Ï†ê (Perfect, 100Ï†ê)
        # - 75%+ ‚Üí 2Ï†ê (Good, 66.7Ï†ê)
        # - 60%+ ‚Üí 1Ï†ê (Needs improvement, 33.3Ï†ê)
        # - <60% ‚Üí 0Ï†ê (Incorrect or no movement, 0Ï†ê)
        # ========================================================================
        if probability >= 0.90:
            return 3
        if probability >= 0.75:
            return 2
        if probability >= 0.60:
            return 1
        return 0

    def _fallback_score(
        self, predicted_label: str, confidence: float, target_action: str | None
    ) -> int:
        # ========================================================================
        # Fallback scoring when target_probability is not available
        # Aligned with stricter _score_by_probability criteria
        # ========================================================================
        if not target_action:
            # No target specified - use general confidence thresholds
            if confidence >= 0.90:
                return 3
            if confidence >= 0.75:
                return 2
            if confidence >= 0.60:
                return 1
            return 0

        target_key = target_action.strip().upper()
        predicted_key = predicted_label.strip().upper()

        if target_key == predicted_key:
            # Predicted correctly - use confidence thresholds
            if confidence >= 0.90:
                return 3
            if confidence >= 0.75:
                return 2
            if confidence >= 0.60:
                return 1
            return 0
        else:
            # Predicted incorrectly - always 0 points
            return 0

    # ========================================================================
    # ‚ö†Ô∏è CRITICAL: Filter out invalid frames (zero vectors) to prevent
    # meaningless predictions when person is not detected by Mediapipe
    # ========================================================================
    # Issue: When user doesn't move or is out of frame, Mediapipe returns
    # zero vectors, but model still predicts with low confidence (~15-20%)
    # This causes unfair scoring where "no movement" gets ~33-50 points!
    #
    # Solution: Only use frames where person is actually detected
    # Require minimum 3 valid frames to ensure reliable prediction
    # ========================================================================
    def _frames_to_keypoints(self, frames: Iterable[str]) -> Tuple[np.ndarray, float, float]:
        keypoints = []
        valid_count = 0
        total_count = 0

        decode_elapsed = 0.0
        pose_elapsed = 0.0
        for encoded in frames:
            total_count += 1
            decode_start = perf_counter()
            image = self._decode_base64_image(encoded)
            decode_elapsed += perf_counter() - decode_start

            pose_start = perf_counter()
            coords = self.pose_extractor.extract(image)
            pose_elapsed += perf_counter() - pose_start

            # Skip zero vectors (person not detected)
            if np.any(coords):
                keypoints.append(coords)
                valid_count += 1

        # Require at least 5 valid frames for reliable prediction
        # (Increased from 3 to reduce false positives when stationary)
        MIN_VALID_FRAMES = 5
        if valid_count < MIN_VALID_FRAMES:
            raise ValueError(
                f"Ïú†Ìö®Ìïú ÎèôÏûë ÌîÑÎ†àÏûÑÏù¥ Î∂ÄÏ°±Ìï©ÎãàÎã§ ({valid_count}/{total_count}Í∞ú). "
                f"Ïπ¥Î©îÎùºÏóê Ï†ÑÏã†Ïù¥ Î≥¥Ïù¥ÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî."
            )

        LOGGER.info(
            "üìπ ÌîÑÎ†àÏûÑ Î∂ÑÏÑù: Ïú†Ìö®=%dÍ∞ú, Ï†ÑÏ≤¥=%dÍ∞ú, ÌïÑÌÑ∞ÎßÅ=%dÍ∞ú (ÏòÅÎ≤°ÌÑ∞ Ï†úÏô∏)",
            valid_count, total_count, total_count - valid_count
        )

        keypoint_array = np.stack(keypoints, axis=0).astype(np.float32)
        return keypoint_array, decode_elapsed, pose_elapsed

    def _sample_frames(self, frames: Sequence[str], target_count: int) -> List[str]:
        if len(frames) == target_count:
            return list(frames)

        if len(frames) < target_count:
            padding = [frames[-1]] * (target_count - len(frames))
            return list(frames) + padding

        indices = np.linspace(0, len(frames) - 1, target_count).astype(int)
        return [frames[i] for i in indices]

    @staticmethod
    def _decode_base64_image(data: str) -> np.ndarray:
        """
        Decode base64 image and apply EXIF orientation correction.

        EXIF orientation tags can indicate image rotation/flip.
        If not corrected, MediaPipe will extract landmarks from rotated image,
        causing misaligned predictions.
        """
        try:
            image_data = base64.b64decode(data)
        except base64.binascii.Error as exc:
            raise ValueError("Base64 ÎîîÏΩîÎî©Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.") from exc

        # ========================================================================
        # ‚ö†Ô∏è CRITICAL: Apply EXIF orientation correction
        # ========================================================================
        # PIL's ImageOps.exif_transpose() auto-rotates based on EXIF orientation
        # This ensures consistency with training data preprocessing
        # ========================================================================
        from PIL import ImageOps
        with Image.open(BytesIO(image_data)) as img:
            img = ImageOps.exif_transpose(img)
            if img is None:
                # If exif_transpose returns None (no EXIF), reload original
                img = Image.open(BytesIO(image_data))
            rgb_image = img.convert("RGB")
            return np.array(rgb_image)


@lru_cache(maxsize=1)
def get_inference_service() -> MotionInferenceService:
    model_path = Path(__file__).resolve().parent.parent / "trained_model" / "gcn_cnn_best.pt"
    device_override = os.getenv("MOTION_INFERENCE_DEVICE")
    return MotionInferenceService(model_path=model_path, device=device_override)


