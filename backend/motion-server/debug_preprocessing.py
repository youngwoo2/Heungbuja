"""
í•™ìŠµ ë°ì´í„°ì™€ ì¶”ë¡  ì…ë ¥ ì „ì²˜ë¦¬ ë¹„êµ ë””ë²„ê¹…

í•™ìŠµ ì‹œ ì‚¬ìš©í•œ .npz íŒŒì¼ê³¼ ì‹¤ì œ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ poseë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import base64
import glob
import os
from pathlib import Path

import numpy as np
from PIL import Image

from app.services.inference import PoseExtractor


def load_training_npz_sample():
    """í•™ìŠµì— ì‚¬ìš©ëœ .npz ìƒ˜í”Œ í•˜ë‚˜ ë¡œë“œ"""
    npz_files = glob.glob("app/brandnewTrain/pose_sequences/JSY/CLAP/*.npz")

    if not npz_files:
        print("âŒ .npz íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None

    npz_path = npz_files[0]
    print(f"ğŸ“‚ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ: {npz_path}")

    data = np.load(npz_path)
    landmarks = data["landmarks"]  # (T, 33, 3 or 4)

    print(f"   Shape: {landmarks.shape}")
    print(f"   Frames: {len(landmarks)}")

    return landmarks


def load_inference_image_sample():
    """ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ì´ë¯¸ì§€ ìƒ˜í”Œ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    image_files = sorted(glob.glob("app/brandnewTrain/extracted_data/JSY/CLAP/clap_seq001_frame*.jpg"))
    image_files = [f for f in image_files if "_backup" not in f]

    if len(image_files) < 8:
        print("âŒ ì´ë¯¸ì§€ íŒŒì¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
        return None

    print(f"ğŸ“¸ ì¶”ë¡  ì´ë¯¸ì§€ ìƒ˜í”Œ: clap_seq001 (8 frames)")

    pose_extractor = PoseExtractor()
    keypoints_list = []

    for img_path in image_files[:8]:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(img_path)
        image_np = np.array(image)

        # Pose ì¶”ì¶œ (ì „ì²˜ë¦¬ í¬í•¨)
        keypoints = pose_extractor.extract(image_np)  # (22, 2)
        keypoints_list.append(keypoints)

    keypoints_array = np.stack(keypoints_list, axis=0)  # (8, 22, 2)
    print(f"   Shape: {keypoints_array.shape}")
    print(f"   Frames: {len(keypoints_array)}")

    return keypoints_array


def compare_preprocessing():
    """í•™ìŠµ ë°ì´í„°ì™€ ì¶”ë¡  ë°ì´í„°ì˜ ì „ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ"""
    print("\n" + "=" * 80)
    print("ğŸ” ì „ì²˜ë¦¬ ë¹„êµ ë¶„ì„")
    print("=" * 80 + "\n")

    # í•™ìŠµ ë°ì´í„°
    training_landmarks = load_training_npz_sample()
    if training_landmarks is None:
        return

    # í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
    from app.brandnewTrain.train_gcn_cnn import normalize_landmarks, USED_LANDMARK_INDICES

    training_normalized = normalize_landmarks(training_landmarks)  # (T, 22, 2)
    print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   Shape: {training_normalized.shape}")
    print(f"   Mean: {training_normalized.mean():.6f}")
    print(f"   Std: {training_normalized.std():.6f}")
    print(f"   Min: {training_normalized.min():.6f}")
    print(f"   Max: {training_normalized.max():.6f}")
    print(f"   ì²« í”„ë ˆì„ ì²« í‚¤í¬ì¸íŠ¸: {training_normalized[0, 0]}")

    # ì¶”ë¡  ë°ì´í„°
    print("\n")
    inference_keypoints = load_inference_image_sample()
    if inference_keypoints is None:
        return

    print(f"\nğŸ“Š ì¶”ë¡  ë°ì´í„° ì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   Shape: {inference_keypoints.shape}")
    print(f"   Mean: {inference_keypoints.mean():.6f}")
    print(f"   Std: {inference_keypoints.std():.6f}")
    print(f"   Min: {inference_keypoints.min():.6f}")
    print(f"   Max: {inference_keypoints.max():.6f}")
    print(f"   ì²« í”„ë ˆì„ ì²« í‚¤í¬ì¸íŠ¸: {inference_keypoints[0, 0]}")

    # ë¹„êµ
    print("\n" + "=" * 80)
    print("ğŸ“Š í†µê³„ ë¹„êµ:")
    print("=" * 80)

    print(f"\nMean ì°¨ì´: {abs(training_normalized.mean() - inference_keypoints.mean()):.6f}")
    print(f"Std ì°¨ì´: {abs(training_normalized.std() - inference_keypoints.std()):.6f}")

    # ê°’ ë²”ìœ„ ë¹„êµ
    print(f"\ní•™ìŠµ ë°ì´í„° ë²”ìœ„: [{training_normalized.min():.4f}, {training_normalized.max():.4f}]")
    print(f"ì¶”ë¡  ë°ì´í„° ë²”ìœ„: [{inference_keypoints.min():.4f}, {inference_keypoints.max():.4f}]")

    # Shape ë¹„êµ
    print(f"\ní•™ìŠµ ë°ì´í„° Shape: {training_normalized.shape}")
    print(f"ì¶”ë¡  ë°ì´í„° Shape: {inference_keypoints.shape}")

    if training_normalized.shape[1:] != inference_keypoints.shape[1:]:
        print("\nâš ï¸  ê²½ê³ : Shapeê°€ ë‹¤ë¦…ë‹ˆë‹¤!")
        print(f"   í•™ìŠµ: (frames, {training_normalized.shape[1]}, {training_normalized.shape[2]})")
        print(f"   ì¶”ë¡ : (frames, {inference_keypoints.shape[1]}, {inference_keypoints.shape[2]})")

    # 0 ë²¡í„° í™•ì¸
    training_zeros = np.all(training_normalized == 0, axis=-1).sum()
    inference_zeros = np.all(inference_keypoints == 0, axis=-1).sum()

    print(f"\n0 ë²¡í„° ê°œìˆ˜:")
    print(f"   í•™ìŠµ ë°ì´í„°: {training_zeros} / {training_normalized.shape[0] * training_normalized.shape[1]}")
    print(f"   ì¶”ë¡  ë°ì´í„°: {inference_zeros} / {inference_keypoints.shape[0] * inference_keypoints.shape[1]}")

    print("\n" + "=" * 80 + "\n")


if __name__ == "__main__":
    compare_preprocessing()
